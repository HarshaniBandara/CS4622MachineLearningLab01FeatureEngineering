# -*- coding: utf-8 -*-
"""label_2_lab_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18uJx21lUvtTKmp3948QWD7K8F4Udm-8K
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold
# Specify the path to your CSV file
standard_data_path= '/content/drive/My Drive/Semester 7/machine learning/lab_1/train.csv'
csv_path_of_test_data='/content/drive/My Drive/Semester 7/machine learning/lab_1/valid.csv'
# Read the CSV file using pandas
standard_data = pd.read_csv(standard_data_path)
test_data=pd.read_csv(csv_path_of_test_data)
# Display the first few rows of the DataFrame
standard_data.head()

standard_data.shape
standard_data.drop(columns=["label_1","label_3","label_4"],axis=1,inplace=True)
test_data.drop(columns=["label_1","label_3","label_4"],axis=1,inplace=True)

# preprocessing data because there are some NaN values\
columns_with_missing = standard_data.columns[standard_data.isna().any()].tolist()
cleaned_standard_data = standard_data.dropna(subset=columns_with_missing)

columns_with_missing = test_data.columns[test_data.isna().any()].tolist()
test_data = test_data.dropna(subset=columns_with_missing)

x_train=cleaned_standard_data.iloc[:,:256]
y_train=cleaned_standard_data.iloc[:,256:257]

x_test=test_data.iloc[:,:256]
y_test=test_data.iloc[:,256:257]
print(x_test.shape,y_test.shape)



import numpy as np
import pandas as pd
import xgboost as xg
from sklearn.metrics import mean_squared_error as MSE

from sklearn.metrics import r2_score
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Initialize the xg-boost
xgb_r = xg.XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)

# Train the model
xgb_r.fit(x_train, y_train)

# Make predictions on the test set
y_pred = xgb_r.predict(x_test)
y_pred_before=y_pred
mse = MSE(y_test, y_pred)
print("Mean Squared Error:", mse)



import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
print(y_train.describe())

# Distribution plot
plt.figure(figsize=(8, 6))
sns.histplot(y_train, bins=10, kde=True)
plt.title("Distribution of Target Variable")
plt.xlabel("Target Value")
plt.ylabel("Frequency")
plt.show()

# Box plot
plt.figure(figsize=(8, 6))
sns.boxplot(y_train)
plt.title("Box Plot of Target Variable")
plt.xlabel("Target Value")
plt.show()

# Scatter plot (if applicable, to visualize relationships with other features)
plt.figure(figsize=(8, 6))
sns.scatterplot(x=x_train['feature_1'], y=y_train['label_2'])
plt.title("Scatter Plot: Target vs Feature1")
plt.xlabel("Feature1")
plt.ylabel("Target Value")
plt.show()

from sklearn.preprocessing import StandardScaler

scaler=StandardScaler()
x_scaled=scaler.fit_transform(x_train)
#  convert X_scaled back to a pandas DataFrame
x_train_scaled_df = pd.DataFrame(x_scaled, columns=x_train.columns)
x_train_scaled_df=x_train

scaler=StandardScaler()
x_test_scaled=scaler.fit_transform(x_test)
#  convert X_scaled back to a pandas DataFrame
x_test_scaled_df = pd.DataFrame(x_test_scaled, columns=x_train.columns)
x_test_scaled_df=x_test

"""xgboost for scaled data set"""

import numpy as np
import pandas as pd
import xgboost as xg
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error as MSE


from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import hamming_loss, jaccard_score
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Initialize the xg-boost
xgb_r = xg.XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)

# Train the model
xgb_r.fit(x_train_scaled_df, y_train)

# Make predictions on the test set
y_pred = xgb_r.predict(x_test_scaled_df)
mse = MSE(y_test, y_pred)
print("Mean Squared Error:", mse)

correlated_features = set()
correlation_matrix =x_train_scaled_df.corr()
x_train_scaled_df.corr()

for i in range(len(correlation_matrix .columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > 0.6:
            colname = correlation_matrix.columns[i]
            correlated_features.add(colname)

len(correlated_features)

print(correlated_features)

# x_train_scaled_df.drop(columns=correlated_features, axis=1, inplace=True)
# x_test_scaled_df.drop(columns=correlated_features, axis=1, inplace=True)

# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.metrics import hamming_loss, jaccard_score

# # Initialize the xg-boost
# xgb_r = xg.XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)

# # Train the model
# xgb_r.fit(x_train_scaled_df, y_train)

# # Make predictions on the test set
# y_pred = xgb_r.predict(x_test_scaled_df)
# mse = MSE(y_test, y_pred)
# print("Mean Squared Error:", mse)

from sklearn.decomposition import PCA

pca=PCA(.97, svd_solver='full')
pca=pca.fit(x_train_scaled_df)
x_train_pca=pca.transform(x_train_scaled_df)
x_test_pca=pca.transform(x_test_scaled_df)
print(x_test_pca.shape)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import hamming_loss, jaccard_score,r2_score

import numpy as np
import pandas as pd
import xgboost as xg
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error as MSE


# Initialize the xg-boost
xgb_r = xg.XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)

# Train the model
xgb_r.fit(x_train_pca, y_train)

# Make predictions on the test set
y_pred = xgb_r.predict(x_test_pca)
y_pred_after=y_pred
mse = MSE(y_test, y_pred)
r2=r2_score(y_test,y_pred)
print("Mean Squared Error:", mse)
print('r2 score: ',r2)

pca_columns = [f'New_feature_{i+1}' for i in range(x_train_pca.shape[1])]
df_final_Label1 = pd.DataFrame(x_test_pca, columns=pca_columns)
df_final_Label1.insert(0, 'Label_1_predictions_before', y_pred_before)
df_final_Label1.insert(1, 'Label_1_predictions_after',y_pred_after)
df_final_Label1.insert(2,'No of new features', [pca.n_components_ for i in range(len(y_pred_before))])
df_final_Label1.to_csv('190088H_Label_2.csv')

# !pip install shap
# import xgboost
# import shap
# from sklearn.ensemble import RandomForestClassifier
# model = RandomForestClassifier()

# model.fit(x_train_scaled_df, y_train)



# explainer = shap.TreeExplainer(model)
# shap_values = explainer.shap_values(x_train_scaled_df)
# shap_feature_importance = abs(shap_values).mean(axis=0)
# important_features = x_train_scaled_df.columns[shap_feature_importance > 0.9]

