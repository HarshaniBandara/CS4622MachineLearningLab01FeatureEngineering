# -*- coding: utf-8 -*-
"""label _1_lab_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lstzQ9XQ7wyrE15Paixhk-OJrdX5rkIO
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold
# Specify the path to your CSV file
standard_data_path= '/content/drive/My Drive/Semester 7/machine learning/lab_1/train.csv'
csv_path_of_test_data='/content/drive/My Drive/Semester 7/machine learning/lab_1/valid.csv'
# Read the CSV file using pandas
standard_data = pd.read_csv(standard_data_path)
test_data=pd.read_csv(csv_path_of_test_data)
# Display the first few rows of the DataFrame
standard_data.head()

standard_data.shape
standard_data.drop(columns=["label_2","label_3","label_4"],axis=1,inplace=True)
test_data.drop(columns=["label_2","label_3","label_4"],axis=1,inplace=True)
standard_data
test_data

x=standard_data.iloc[:,:256]
y=standard_data.iloc[:,256:257]

x_test=test_data.iloc[:,:256]
y_test=test_data.iloc[:,256:257]
print(x_test.shape,y_test.shape)
print(x.columns.tolist())

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


# Calculate label correlations
column_to_visualize = 'label_1'

# Create a histogram
plt.hist(y[column_to_visualize], bins=20, edgecolor='k')
plt.xlabel(column_to_visualize)
plt.ylabel('Frequency')
plt.title(f'Distribution of {column_to_visualize}')
plt.show()

frequency_counts = y['label_1'].value_counts()

#use knn without preprocessing
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import hamming_loss, jaccard_score
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Initialize the k-NN classifier
knn = KNeighborsClassifier(n_neighbors=5)

# Train the model
knn.fit(x, y)

# Make predictions on the test set
y_pred = knn.predict(x_test)
print(accuracy_score(y_test, y_pred))
print(classification_report(y_test,y_pred))

"""scalling the x data set"""



from sklearn.preprocessing import StandardScaler

scaler=StandardScaler()
x_scaled=scaler.fit_transform(x)
#  convert X_scaled back to a pandas DataFrame
x_scaled_df = pd.DataFrame(x_scaled, columns=x.columns)
x_scaled_df

scaler=StandardScaler()
x_test_scaled=scaler.fit_transform(x_test)
#  convert X_scaled back to a pandas DataFrame
x_test_scaled_df = pd.DataFrame(x_test_scaled, columns=x.columns)
x_test_scaled_df

constant_filter = VarianceThreshold(threshold=0)
constant_filter.fit(x_scaled_df)
train_features=x_scaled_df
train_labels=y
test_features=x_test_scaled_df
test_labels=y_test

len(train_features.columns[constant_filter.get_support()])

constant_columns = [column for column in train_features.columns
                    if column not in train_features.columns[constant_filter.get_support()]]

print(len(constant_columns))

for column in constant_columns:
    print(column)

train_features = constant_filter.transform(train_features)
test_features = constant_filter.transform(test_features)

train_features.shape, test_features.shape

print(train_features)



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import hamming_loss, jaccard_score

# Split data into training and testing sets

# X_train, X_test, y_train, y_test = train_test_split(trai_features, y_imputed_label_1, test_size=0.2, random_state=42)

# Initialize the k-NN classifier
knn = KNeighborsClassifier(n_neighbors=5)

# Train the model
knn.fit(train_features, train_labels)

# Make predictions on the test set
y_pred = knn.predict(test_features)
y_pred_before=y_pred

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
print(accuracy_score(test_labels, y_pred))
print(classification_report(test_labels,y_pred))

"""there are no fetures that has 0 varience, so any fetures not removed.then try to remove Quasi-constant features"""

qconstant_filter = VarianceThreshold(threshold=0.01)
qconstant_filter.fit(train_features)
train_features= pd.DataFrame(train_features, columns=x.columns)
test_features=pd.DataFrame(test_features, columns=x.columns)

len(train_features.columns[qconstant_filter.get_support()])

qconstant_columns = [column for column in train_features.columns
                    if column not in train_features.columns[qconstant_filter.get_support()]]

print(len(qconstant_columns))

"""there are no qusai- features, so then try to remove duplicate features"""

train_features_T = train_features.T
train_features_T.shape
print(train_features_T.duplicated().sum())

"""No duplicate features area found

try to remove correlated features
"""

correlated_features = set()
correlation_matrix =x.corr()

for i in range(len(correlation_matrix .columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > 0.4:
            colname = correlation_matrix.columns[i]
            correlated_features.add(colname)

len(correlated_features)

print(correlated_features)

train_features.drop(columns=correlated_features, axis=1, inplace=True)
test_features.drop(columns=correlated_features, axis=1, inplace=True)
x_scaled_df.drop(columns=correlated_features, axis=1, inplace=True)

# Initialize the k-NN classifier
knn = KNeighborsClassifier(n_neighbors=5)

# Train the model
knn.fit(train_features, train_labels)

# Make predictions on the test set
y_pred = knn.predict(test_features)

print(accuracy_score(test_labels, y_pred))
print(classification_report(test_labels,y_pred))

# feature imporatance method only support for random forest
#  importance=knn.feature_importances_
# train_features_copy = train_features.copy()
# test_features_copy = test_features.copy()
# for i,v in enumerate(importance):
#   if v < 0.004:
#     train_features_copy.drop(columns=train_features.columns[i], inplace=True)
#     test_features_copy.drop(columns=test_features.columns[i], inplace=True)

from sklearn.decomposition import PCA

pca=PCA(.95, svd_solver='full')
pca=pca.fit(train_features)
train_features_pca=pca.transform(train_features)
test_features_pca=pca.transform(test_features)

pca.n_components_
train_features_pca.shape

knn = KNeighborsClassifier(n_neighbors=5)

# Train the model
knn.fit(train_features_pca, train_labels)

# Make predictions on the test set
y_pred = knn.predict(test_features_pca)
y_pred_after=y_pred

# print(accuracy_score(test_labels, y_pred))
# print(classification_report(test_labels,y_pred))

pca_columns = [f'New_feature_{i+1}' for i in range(train_features_pca.shape[1])]
df_final_Label1 = pd.DataFrame(test_features_pca, columns=pca_columns)
df_final_Label1.insert(0, 'Label_1_predictions_before', y_pred_before)
df_final_Label1.insert(1, 'Label_1_predictions_after',y_pred_after)
df_final_Label1.insert(2,'No of new features', [pca.n_components_ for i in range(len(y_pred_before))])
df_final_Label1.to_csv('190088H_Label_1.csv')

