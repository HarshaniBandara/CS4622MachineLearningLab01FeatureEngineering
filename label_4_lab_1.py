# -*- coding: utf-8 -*-
"""label_4_lab_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hZOp8jEPgIhwTExcJAz35jPuJVxEhKL9
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold
# Specify the path to your CSV file
standard_data_path= '/content/drive/My Drive/Semester 7/machine learning/lab_1/train.csv'
csv_path_of_test_data='/content/drive/My Drive/Semester 7/machine learning/lab_1/valid.csv'
# Read the CSV file using pandas
standard_data = pd.read_csv(standard_data_path)
test_data=pd.read_csv(csv_path_of_test_data)
# Display the first few rows of the DataFrame
standard_data.head()

standard_data.shape
standard_data.drop(columns=["label_1","label_2","label_3"],axis=1,inplace=True)
test_data.drop(columns=["label_1","label_2","label_3"],axis=1,inplace=True)



# preprocessing data because there are some NaN values\
columns_with_missing = standard_data.columns[standard_data.isna().any()].tolist()
cleaned_standard_data = standard_data.dropna(subset=columns_with_missing)

x_train=cleaned_standard_data.iloc[:,:256]
y_train=cleaned_standard_data.iloc[:,256:257]

x_test=test_data.iloc[:,:256]
y_test=test_data.iloc[:,256:257]
print(x_test.shape,y_test.shape)

#use knn without preprocessing
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import hamming_loss, jaccard_score
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Initialize the k-NN classifier
knn = KNeighborsClassifier(n_neighbors=5)

# Train the model
knn.fit(x_train, y_train)

# Make predictions on the test set
y_pred = knn.predict(x_test)
y_pred_before=knn.predict(x_test)
print(accuracy_score(y_test, y_pred))
print(classification_report(y_test,y_pred))

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


# Calculate label correlations
column_to_visualize = 'label_4'

# Create a histogram
plt.hist(y_train[column_to_visualize], bins=20, edgecolor='k')
plt.xlabel(column_to_visualize)
plt.ylabel('Frequency')
plt.title(f'Distribution of {column_to_visualize}')
plt.show()

frequency_counts = y_train['label_4'].value_counts()
print(frequency_counts)

"""Resampling because data set is highly imbalanced"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
from sklearn.metrics import classification_report


from imblearn.over_sampling import RandomOverSampler
# # Apply SMOTE to oversample the minority classes
# smote = SMOTE(sampling_strategy='auto', random_state=42)
# x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)


over_sampler = RandomOverSampler(sampling_strategy='auto')
x_train_resampled, y_train_resampled = over_sampler.fit_resample(x_train, y_train)

# after sampling
# Calculate label correlations
column_to_visualize = 'label_4'

# Create a histogram
plt.hist(y_train_resampled[column_to_visualize], bins=20, edgecolor='k')
plt.xlabel(column_to_visualize)
plt.ylabel('Frequency')
plt.title(f'Distribution of {column_to_visualize}')
plt.show()

frequency_counts = y_train['label_4'].value_counts()

"""scalling"""

from sklearn.preprocessing import StandardScaler


from sklearn.preprocessing import StandardScaler as ss
scaler = ss()
scaler.fit(x_train_resampled)

x_train_scaled_df= scaler.transform(x_train_resampled)
x_test_scaled_df = scaler.transform(x_test)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import hamming_loss, jaccard_score

# Initialize the k-NN classifier
knn = KNeighborsClassifier(n_neighbors=5)

# Train the model
knn.fit(x_train_scaled_df, y_train_resampled)

# Make predictions on the test set
y_pred = knn.predict(x_test_scaled_df)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
print(accuracy_score(y_test, y_pred))
print(classification_report(y_test,y_pred))

correlated_features = set()
correlation_matrix =x_train.corr()
x_train.corr()

for i in range(len(correlation_matrix .columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > 0.4:
            colname = correlation_matrix.columns[i]
            correlated_features.add(colname)

len(correlated_features)

print(correlated_features)

# x_train_scaled_df.drop(columns=correlated_features, axis=1, inplace=True)
# x_test_scaled_df.drop(columns=correlated_features, axis=1, inplace=True)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import hamming_loss, jaccard_score

# Split data into training and testing sets

# X_train, X_test, y_train, y_test = train_test_split(trai_features, y_imputed_label_1, test_size=0.2, random_state=42)

# Initialize the k-NN classifier
knn = KNeighborsClassifier(n_neighbors=5)

# Train the model
knn.fit(x_train_scaled_df, y_train_resampled)

# Make predictions on the test set
y_pred = knn.predict(x_test_scaled_df)
print(accuracy_score(y_test, y_pred))
print(classification_report(y_test,y_pred))

from sklearn.decomposition import PCA

pca=PCA(.98, svd_solver='full')
pca=pca.fit(x_train_scaled_df)
x_train_pca=pca.transform(x_train_scaled_df)
x_test_pca=pca.transform(x_test_scaled_df)
print(x_test_pca.shape)

# X_train, X_test, y_train, y_test = train_test_split(trai_features, y_imputed_label_1, test_size=0.2, random_state=42)

# Initialize the k-NN classifier
knn = KNeighborsClassifier(n_neighbors=5)

# Train the model
knn.fit(x_train_pca, y_train_resampled)

# Make predictions on the test set
y_pred = knn.predict(x_test_pca)
y_pred_after=knn.predict(x_test_pca)
print(accuracy_score(y_test, y_pred))
print(classification_report(y_test,y_pred))

pca_pca_columns = [f'New_feature_{i+1}' for i in range(x_train_pca.shape[1])]
df_final_Label1 = pd.DataFrame(x_test_pca, columns=pca_pca_columns)
df_final_Label1.insert(0, 'Label_1_predictions_before', y_pred_before)
df_final_Label1.insert(1, 'Label_1_predictions_after',y_pred_after)
df_final_Label1.insert(2,'No of new features', [pca.n_components_ for i in range(len(y_pred_before))])
df_final_Label1.to_csv('190088H_Label_4.csv')







