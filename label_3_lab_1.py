# -*- coding: utf-8 -*-
"""label_3_lab_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QXQ9CsX7E0iOtB73CZAVJi_4hp_Iuc1k
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold
# Specify the path to your CSV file
standard_data_path= '/content/drive/My Drive/Semester 7/machine learning/lab_1/train.csv'
csv_path_of_test_data='/content/drive/My Drive/Semester 7/machine learning/lab_1/valid.csv'
# Read the CSV file using pandas
standard_data = pd.read_csv(standard_data_path)
test_data=pd.read_csv(csv_path_of_test_data)
# Display the first few rows of the DataFrame
standard_data.head()

standard_data.shape
standard_data.drop(columns=["label_1","label_2","label_4"],axis=1,inplace=True)
test_data.drop(columns=["label_1","label_2","label_4"],axis=1,inplace=True)
print(standard_data)
print(test_data)

# preprocessing data because there are some NaN values\
columns_with_missing = standard_data.columns[standard_data.isna().any()].tolist()
cleaned_standard_data = standard_data.dropna(subset=columns_with_missing)

x_train=cleaned_standard_data.iloc[:,:256]
y_train=cleaned_standard_data.iloc[:,256:257]

x_test=test_data.iloc[:,:256]
y_test=test_data.iloc[:,256:257]
print(x_test.shape,y_test.shape)

#use knn without preprocessing
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import hamming_loss, jaccard_score
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Initialize the k-NN classifier
knn = KNeighborsClassifier(n_neighbors=5)

# Train the model
knn.fit(x_train, y_train)

# Make predictions on the test set
y_pred = knn.predict(x_test)
print(accuracy_score(y_test, y_pred))
print(classification_report(y_test,y_pred))

from sklearn.preprocessing import StandardScaler

scaler=StandardScaler()
x_scaled=scaler.fit_transform(x_train)
#  convert X_scaled back to a pandas DataFrame
x_train_scaled_df = pd.DataFrame(x_scaled, columns=x_train.columns)
x_train_scaled_df

scaler=StandardScaler()
x_test_scaled=scaler.fit_transform(x_test)
#  convert X_scaled back to a pandas DataFrame
x_test_scaled_df = pd.DataFrame(x_test_scaled, columns=x_train.columns)
x_test_scaled_df

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import hamming_loss, jaccard_score

# Split data into training and testing sets

# X_train, X_test, y_train, y_test = train_test_split(trai_features, y_imputed_label_1, test_size=0.2, random_state=42)

# Initialize the k-NN classifier
knn = KNeighborsClassifier(n_neighbors=5)

# Train the model
knn.fit(x_train_scaled_df, y_train)

# Make predictions on the test set
y_pred = knn.predict(x_test_scaled)
y_pred_before=y_pred

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
print(accuracy_score(y_test, y_pred))
print(classification_report(y_test,y_pred))

correlated_features = set()
correlation_matrix =x_train_scaled_df.corr()
x_train_scaled_df.corr()

for i in range(len(correlation_matrix .columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > 0.4:
            colname = correlation_matrix.columns[i]
            correlated_features.add(colname)

len(correlated_features)

print(correlated_features)

x_train_scaled_df.drop(columns=correlated_features, axis=1, inplace=True)
x_test_scaled_df.drop(columns=correlated_features, axis=1, inplace=True)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import hamming_loss, jaccard_score

# Split data into training and testing sets

# X_train, X_test, y_train, y_test = train_test_split(trai_features, y_imputed_label_1, test_size=0.2, random_state=42)

# Initialize the k-NN classifier
knn = KNeighborsClassifier(n_neighbors=5)

# Train the model
knn.fit(x_train_scaled_df, y_train)

# Make predictions on the test set
y_pred = knn.predict(x_test_scaled_df)
print(accuracy_score(y_test, y_pred))
print(classification_report(y_test,y_pred))

from sklearn.decomposition import PCA

pca=PCA(.97, svd_solver='full')
pca=pca.fit(x_train_scaled_df)
x_train_pca=pca.transform(x_train_scaled_df)
x_test_pca=pca.transform(x_test_scaled_df)
x_test_pca.shape

# X_train, X_test, y_train, y_test = train_test_split(trai_features, y_imputed_label_1, test_size=0.2, random_state=42)

# Initialize the k-NN classifier
knn = KNeighborsClassifier(n_neighbors=5)

# Train the model
knn.fit(x_train_pca, y_train)

# Make predictions on the test set
y_pred = knn.predict(x_test_pca)
y_pred_after=y_pred
print(accuracy_score(y_test, y_pred))
print(classification_report(y_test,y_pred))

pca_pca_columns = [f'New_feature_{i+1}' for i in range(x_train_pca.shape[1])]
df_final_Label1 = pd.DataFrame(x_test_pca, columns=pca_pca_columns)
df_final_Label1.insert(0, 'Label_1_predictions_before', y_pred_before)
df_final_Label1.insert(1, 'Label_1_predictions_after',y_pred_after)
df_final_Label1.insert(2,'No of new features', [pca.n_components_ for i in range(len(y_pred_before))])
df_final_Label1.to_csv('190088H_Label_3.csv')